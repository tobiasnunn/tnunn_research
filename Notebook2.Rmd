---
title: "ENS-2002 Scientific Notebook MK2 number 2"
author: "Tobias Nunn"
date: "2025-01-13"
output: 
  html_document: 
    toc: true
    toc_depth: 1
    keep_md: true
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<style>
    body .main-container {
        max-width: 1200px;
    }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = T)
library(dplyr)
library(tidyr)
library(tidyverse)
library(gt)
library(knitr)
library(jsonlite)
library(tidyjson)
library(patchwork)
library(gtExtras)
```

# EggNog-Mapper initial test analyses (2025-01-10 to 2025-01-19)

## Introduction

Aaron emailed me back with advice on a way to make eggNog-mapper/2.1.12
on hawk work. As i am revising for an exam i can't spend too long on
this. p.s. i think i did pretty good on the exam. This section is
somewhat of a proof of concept just so i can understand and play with
the base commands for eggnogmapepr and try to get it running quickly,
and all the trials that entails. Hawk is still running slow, so while
those jobs from the previous section are running, i figured i would like
to do something extra to fill time. I wanted to see just how many
samples we are going to need to pull down and process for this and get a
rough time estimate for that, as well as the API stuff to pull the files
down as that is by far the easy bit

## Methods

From the 10th to the 12th i tried running a preliminary [test
script](https://github.com/tobiasnunn/tnunn_research/blob/a962833b3ec1b2c7420fef155b526f2b330d644c/00_scripts/newnogtest.sh)
that I made earlier in december 2024, i may not have written about it as
i didnt get anywhere at that time. This process was also hindered by
hawk being very encumbered in the new year meaning it takes a good half
day for any results to appear. On the 11th i managed to get a successful
result with the .xlsx file i need for the heatmaps for accession 3Dt1c,
found here in a zip file to save on space:

02_middle-analysis_outputs/eggnog_stuff/eggnog_outputs02_middle-analysis_outputs/eggnog_stuff/eggnog_outputs

Today(the 12th) i set off 3 jobs each containing 3 accessions to
hopefully get the results of the remaining 9. If there are no
complications i will then be in a place where i can obtain the .fastas
for the online comparison accessions and then run them through
eggNog-mapper. It should be noted that i used the same list of
parameters i found on the web version of hawk for this.

![Figure 1 - Screenshot of parameters used in early eggnog runs on
hawk](04_images/eggnogparameters.png)

I had some marginal success, the second set of three finished in over 3
hours, the other 2 sets are still going strong after 8, so ill let them
time out over night and see what i have, maybe they will complete, i did
give them 12 hours. One of the remaining two finished just before the
time limit, the other timed out on sample 1Dt100h, proving to be a
difficult sample, this means i have 8 of 10, with the other missing
being 1Dt1h(alphabetically next).

I then discovered the command `dbmem` which could help to speed up the
process. I tested with [this file called
"anothergo.sh"](https://github.com/tobiasnunn/tnunn_research/blob/a962833b3ec1b2c7420fef155b526f2b330d644c/00_scripts/anothergo.sh).
The version linked to is not the file at that point, as i am doing the
linking here retroactively. This was set to run over night from roughly
9 30 pm on the 12th to 3 30 am on the 13th, totalling 6 hours for 5
files, not great. I then experimented with taking out some of the
arguments
`--evalue 0.001 --score 60 --pident 40 --query_cover 20 --subject_cover 20`.
This did not significantly increase time and likely hindered the quality
of the sample, so that was a dead end. Tomorrow(14th) i will have a look
at recreating run 1 with `dbmem` switched on. Around this time i also
tried to run two scripts in parallel, same content but different source
fastas, being another old version of "anothergo.sh" and
"anothergoparallel.sh". This was not successful as slurm scheduled them
sequentially. Today(19th), i changed some criteria around so now there
are 25 cpus per task, however, i believe i had already been doing this
by accident in the previous runs i did. There are two files, one being
the up-to-date version of "anothergo.sh" and one called
[Brachyscript.sh](https://github.com/tobiasnunn/tnunn_research/blob/a962833b3ec1b2c7420fef155b526f2b330d644c/00_scripts/brachy_script.sh)
which did multiple files in sequence but from the same script. I believe
these were not testing any parameters and were just attempts at
preliminary downloads to guage how long it might take. Speed has not
picked up considerably, so i may have reached the limits of how fast i
can make this and thus, it might be time to email Aaron to see what he
thinks. He emailed me back with some ideas about how i could use python
scripts to loop and that could make slurm do them in parallel.

I started this by making tables based along the sets of samples i am
going to need off of the ncbi website. I identified 3 groups of
samples: 1. all the genera in the family sphingomonadaceae / 2. all the
genera in the family Microbacteriaceae / 3. all the genera containing
our flye_asm samples This fits with the specification of work i was
given. Being 2 analyses, 1 for comparing just our genera and another
comparing genera in families we have multiple samples in. As of now i am
yet to do the API call. (still the 17th) I decided to run some more
tests to try cut the time down by adding some more commands to my
anothergo.sh script. I was having trouble with creating the slurm
scripts on hawk, it was a lot of typing complex strings, so i made
another script maker to automate that, which will be much more
convenient when working at scale, here

## Results

Run one had mixed results, 3 sets of 3 ran in parallel, set 1 completed
in 3 and a half hours, good. set 2 took 8 and a half hours, bad, set 3
timed out, very bad. so dont have the outputs needed for 1Dt100h or
1Dt1h. Run 2 was more successful, with 5 solid looking outputs in 6
hours. Run 3 did not improve on that time despite the extra parameters
being cut. As mentioned above, i managed to use `dbmem` and more
optimised cpu and memory usage using the `seff` command. This helped get
the average time to do a file down to roughly 27 minutes. After emailing
Aaron he agreed that the list of accessions should be cut down to remove
groups with low sample counts, i decided to cut at the number 30, so
groups with 29 or below are cut, as seen in these tables:

```{r setup for counts, echo=FALSE}
  datafile <- read.csv("02_middle-analysis_outputs/gtdbtk_stuff/20241224_de_novo_wf/gtdbtk.bac120.decorated.tree-taxonomy", sep = "\t", header = FALSE)
names(datafile) <- c("accession", "path")
datafile <- datafile %>%
  separate_wider_delim(
    path,
    delim = "; ",
    names = c("domain", "phylum", "class", "order", "family", "genus", "species"),
    too_few = "align_start",
    too_many = "merge")
local <- filter(datafile, grepl("flye", accession))
families <- filter(datafile, family %in% local$family)
countinfamily <- filter(families, !family == "f__" ) %>% count(family, genus)
countingenus <- filter(families, genus %in% local$genus) %>% count(genus)
```

### All tables {.tabset .tabset-pills}

#### Family Sphingomonadaceae

```{r chart-f__sphingo, echo=FALSE, warning=FALSE}
filter(countinfamily, family == "f__Sphingomonadaceae"&n>29) %>% gt(groupname_col = "family") %>% opt_row_striping() %>% opt_stylize(style = 5, color = "blue") %>%
  summary_rows(
    #columns = where(is.numeric),
    columns = n,
    fns = list(
      Total = ~sum(., na.rm = TRUE)))  %>%
  tab_header(
    title = "Table 1 - count of genera in the family Sphingomonadaceae",
    subtitle = md("accessions as found in the .tree file outputted by gtdbtk analysis done on 2024-12-24")
  ) %>%
  cols_label(genus = "Family and Genus",
             n = "Number of Accessions") %>%
  tab_footnote(
    footnote="This is a subset of the full list, filtered to those genera with more than 29 accessions.",
    locations = NULL,
    placement = c("auto")
)
```

#### Family Microbacteriaceae

```{r chart-f__Micro, echo=FALSE, warning=FALSE}
filter(countinfamily, family == "f__Microbacteriaceae"&n>29) %>% gt(groupname_col = "family") %>% opt_row_striping() %>% opt_stylize(style = 5, color = "blue") %>%
  summary_rows(
    #columns = where(is.numeric),
    columns = n,
    fns = list(
      Total = ~sum(., na.rm = TRUE))) %>%
  tab_header(
    title = "Table 2 - count of genera in the family Microbacteriaceae",
    subtitle = md("accessions as found in the .tree file outputted by gtdbtk analysis done on 2024-12-24")
  )  %>%
  cols_label(genus = "Family and Genus",
             n = "Number of Accessions") %>%
  tab_footnote(
    footnote="This is a subset of the full list, filtered to those genera with more than 29 accessions.",
    locations = NULL,
    placement = c("auto")
  )
```

#### Our Genera

```{r chart-ourgens, echo=FALSE, warning=FALSE}
countingenus %>% gt() %>% opt_row_striping() %>% opt_stylize(style = 5, color = "blue") %>%
  grand_summary_rows(
    #columns = where(is.numeric),
    columns = n,
    fns = list(
      Total = ~sum(., na.rm = TRUE))) %>%
  tab_header(
    title = "Table 3 - count of genera from accessions produced at Bangor",
    subtitle = md("accessions as found in the .tree file outputted by gtdbtk analysis done on 2024-12-24")
  ) %>%
  cols_label(genus = "Genus",
             n = "Number of Accessions") 
```

###  {.unnumbered}

There are 887 accession in sphingomonadaceae, 806 in microbacteriaceae
and 587 in just our genera, however, there is overlap inside the genera
Sphingomonas and Microbacterium, assuming those values are held inside
the respective numbers before, that comes out to 128 "unique" samples
from that group. This however contains the sample that could not even be
given a family, 1Dt100h, so discounting this odd sample i cannot even
logically analyse 127 accessions. This brings me to the total of 1,820
accessions that need to be processed. The fastest i have been able to
process samples on eggnog is roughly 1 hour and 10 minutes per sample.
Multiplying 1,820 by 1.1667 = 2,123.4 hours. This means that if i could
do them constantly, it would take over 88 days, so 2 months to just run
the samples through eggnog. Through testing i have already made a brief
start, with maybe 20 done.

Right, as of 11:12 today (17th) i have managed to do a successful run
using this file that gave me the output in a nice 25 minutes. I added
some extra fields that i saw both on the online eggnogmapper and from a
stack-overflow from someone doing a similar thing and something worked.
using this new number, 1,820 x 0.4167 = 758.4 hours = 31.6 days so even
now i'm down to just over a month, good stuff. This one used 10 cpus,
the online one used 20, but i didnt want to overdo it with hawk, now im
thinking can i get that number to half again by doubling the cpus?

As of the 24th Aaron agrees with my assessment to cut down on the number
of samples, this leaves me with 1158 samples to run, at an average of 27
minutes, this would take 21.7 days, however, this would speed up
dramatically if i can get his loops working so parallel stuff can
happen.

## Conclusion

It could have been overcrowding on hawk, however it appears that running
multiple sets in parallel adversely affects the result, however, i have
not tested this with `dbmem` on. The large problem is the volume of
samples required, the desired output is 3 heatmaps comparing KO
pathways: - Comparing genera inside sphingomonadaceae - Comparing genera
inside Microbacteriaceae - Comparing the genera containing just our
samples. This number of hours is not one i can logistically work with,
maybe Aaron would be fine with me taking that time, but i feel i can do
it faster. I am still working on improving the script to cut down on
time taken, but hawk is being a pain in that regard, i have also
starting thinking of alternatives or ways to cut down the list. My
front-running idea is to take samples from only genera with more than
10, or 30 or some other arbitrary high number of samples. This would cut
out all groups with only 1-4 accessions in them which i dont think are
useful to this analysis anyway as an average cannot be calculated, it
would also make the heatmaps much more legible, so i might run the
numbers on what cutting down the sample size would look like. It may
however, be better to group all those small genera into an "other"
column to make the dataset as big as possible, but that doesnt solve my
predicament. The total time continues to shrink, with the filter in
place and hopefully with the loops working it could take less than 10
days

::: {style="background-color:yellow;"}
ðŸ“Œ ?: TODO: [eggnog-mapper on hawk is slow, possibly too slow to scale
to where we need it to be. alternatives: \> what scale are we looking
at - 1693 samples for the family comparison \> could limit to genera
with more than 30 samples \> screenscrape-method \> enlist more manpower
to do online(if we have to do on web)]
:::

::: {style="background-color:yellow;"}
ðŸ“Œ ?: TODO: trees and formatting (never did gtdb repeat because need
download all the metadata to make it nice and sparkly)
:::

# A cautionary tale (2025-01-24 11:48)

So for the past 2 weeks while i was doing exams i decided i didnt want
to fully stop this project, but i didnt have enough time to do stuff and
properly fill the notebook, so i decided to leave myself placeholders in
multiple square brackets to make it obvious when i came back later, i
can't replicate this here. For some unknown reason doing too many square
brackets breaks the visual editor and busts the ability to knit. Anyway,
because of that i haveant been able to knit all this time and have just
lost the whole morning to trying to fix this, all over some square
brackets... the shame i feel from this is immeasurable. This really does
prove that there is **always** time for proper note taking.

::: {style="background-color:yellow;"}
ðŸ“Œ ?: TODO: API call for JSONS, fastas
:::

# Prototype Heatmap Creation Run, end-to-end: step 1 2025-01-24 to 2025-01-26

## introduction

Ok, i am done with the eggnog testing, i am in a place now where i am
happy to begin actually doing stuff with this. I figured that i should
start with the less intensive analysis, being the genera comparison of
just the ones our samples are in, seen in Table 3: [Our Genera]. This is
so that i don't have to think about automation to the same degree yet,
thus, i can create the pipeline in a more crude way and add "fancy code
stuff" to make it better later so i'm not wondering whats causing
problems. Even then, i am still going to start smaller, taking only 10
from each genus so that i can create a prototype of sorts, hopefully to
show Aaron when i am back in Bangor. Here is my plan for this section:

1.  Download the metadata for the chosen genera, this is going to use
    the
    [dataset_report](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/api/rest-api/#get-/genome/accession/-accessions-/dataset_report)
    API for the NCBI website. These will serve multiple purposes, for
    example they will tell me which samples have a high quality metric,
    they will also help with the generation of trees from gtdb-tk
    de_novo_wf done in Notebook 1, but not output because of this exact
    reason.

2.  Choose 10 accessions per genus to use in my prototype pipeline. All
    groups will total 10 samples, our own Bangor-made accession will be
    included in this number, excluding 1Dt100h, as gtdb-tk could not
    classify it into a family.

3.  Download the genomic .fasta files for all of these. This will use
    the [Download
    API](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/api/rest-api/#get-/genome/accession/-accessions-/download)
    to pull them from the NCBI website.

4.  Annotate all those fastas using eggNog-mapper on hawk.

5.  create the file(s) that turn the annotation file into a heatmap

## Methods

Using the httr2 package in R, i created . A useful command of not is
"req_dry_run" which allows one to view what the API request looks like.
From the first downloaded JSON(though im sure its in all others this is
the place i first found it) i found the concept of an "ANI" or Average
Nucleotide Identity analysis that the NCBI had performed, this is
interesting, can i do it on mine? One thing i forgot to do was remove
the local samples from the list i passed to NCBI, so those ones
obviously failed but it didn't affect the rest, so if this need be done
again, remember to take the local accessions out of the list. I created
two files for this section of the project:
[NCBImetadatacollector.R](https://github.com/tobiasnunn/tnunn_research/blob/be8100b850eea50a9e3132806623b4f1d1d08956/00_scripts/NCBImetadatacollector.R)
and
[jsondataexplorationprep.R](https://github.com/tobiasnunn/tnunn_research/blob/be8100b850eea50a9e3132806623b4f1d1d08956/00_scripts/jsondataexplorationprep.R).
The former file creates two tables, one containing the genera of
interest to the Bangor genera-genera comparison, and the other results
from using this to call the REST API to the NCBI website and save the
metadata for all the accessions in the aforementioned table. The latter
file manipulates this second table to pull out interesting fields from
the lower levels, combines it with the first to create a sort of
combined table of species and genus information from gtdb-tk combined
with quality metrics from the ncbi.

## Results

The API call itself took around 2 minutes to get 587 samples, very fast,
but also just slow enough to avoid the limit NCBI imposes on download
volume. The ANI stuff in the JSON files was very intriguing for me, i
decided i wanted to output a summary of the JSON files by their ANI
statuses:

### All tables {.tabset .tabset-pills}

#### JSON ANI analysis

```{r ANI and JSON, echo=FALSE, warning=FALSE}

combi <- read.delim("02_middle-analysis_outputs/analysis_tables/genera_analysis_combined.tsv")

# ani count of taxomony_check_status table
anibase <- combi %>% 
  group_by(taxonomy_check_status) %>% 
  summarise(countofsamples = n()) %>% 
  ungroup() %>% 
  arrange(desc(countofsamples)) %>% 
  replace_na(list(taxonomy_check_status = "Absent"))

Taxchektab <- anibase %>%
  gt()  %>%
  tab_source_note(source_note = md("Source: API call to the NCBI website <br> done on 2025-01-24")) %>%
  opt_row_striping() %>%
   opt_stylize(style = 5, color = "blue") %>%
   tab_header(
     title = md("Table 4 - Analysis of <br> Taxonomy Check Status"),
     subtitle = md("as a part of analysis for <br> Average Nuleotide Identity(ANI)")
   ) %>%
  gt_plt_bar(column = countofsamples, keep_column = TRUE, color = "#2565AB") %>%
  cols_label(taxonomy_check_status = "Taxonomy Check Status",
             countofsamples = "Count",
             DUPE_COLUMN_PLT = "Visual") 
Taxchektab

# graph of same thing
# Taxchekgraph <- ggplot(anibase, aes(x = forcats::fct_reorder(taxonomy_check_status, desc(countofsamples)), y = countofsamples)) +
#   geom_col(aes(fill = countofsamples)) + scale_fill_viridis_c() + ylim(0,500) +
#   labs(x = "ANI Taxonomy Check Status", y = "Count of samples",
#        caption = "Source: NCBI REST API done on 2025-01-24") + 
#   geom_text(
#     aes(label = countofsamples),
#     position = position_dodge(width = 1),
#     vjust = -1)
# # combine all the charts into one
# 
# wrap_table(Taxchektab, panel = "full") + Taxchekgraph + plot_annotation(title = "Analysis of Taxonomy Check Status")
  
```

#### ANI match status analysis

```{r Match_Status, echo=FALSE, warning=FALSE}

# ani count of taxomony_check_status table
animatchbase <- combi %>% 
  group_by(taxonomy_check_status, match_status) %>% 
  summarise(countofsamples = n()) %>% 
  ungroup() %>% 
  arrange(desc(countofsamples)) %>% 
  replace_na(list(taxonomy_check_status = "Absent", match_status = "Absent"))
animatchbase$match_status <- factor(animatchbase$match_status, levels = c("species_match", "derived_species_match", "genus_match", 
                                                                          "below_threshold_match", "low_coverage", "below_threshold_lineage_match",
                                                                          "mismatch", "below_threshold_mismatch"))

# #pivot table
# animatchbase %>% pivot_wider(names_from = taxonomy_check_status, values_from = countofsamples)

# stacked graph

ggplot(filter(animatchbase, !taxonomy_check_status == "Absent"), aes(fill= match_status, y=countofsamples, x=taxonomy_check_status)) + 
    geom_bar(position="dodge", stat="identity") + scale_fill_viridis_d() + 
  ylim(0,240) +
  labs(x = "ANI Match Status", y = "Count of samples",
       caption = md("Figure 1: rough analysis of ANI Match Status by \n each stage of the Taxonomy Check Status. \n Source: NCBI REST API done on 2025-01-24"),
       title = "Analysis of ANI Match Status") +
  geom_text(
    aes(label = countofsamples),
    position = position_dodge(width = 1),
    vjust = -1)


```

#### Completeness histogram

```{r completeness hist, echo=FALSE, warning=FALSE}

ggplot(combi) +
  geom_histogram(aes(x = combi$completeness), binwidth = 10, fill = "#2565AB", color = "white") +
  ylim(0,450) +
  labs(x = "Checkm Completeness", y = "Count of samples",
       caption = md("Figure 2: histogram of checkm completeness from analysis done by the NCBI at an undisclosed date"),
       title = "Analysis Checkm Completeness")

 +
  geom_text(
    aes(label = completeness),
    position = position_dodge(width = 1),
    vjust = -1)
```

#### Contamination histogram

```{r contamination hist, echo=FALSE, warning=FALSE}

ggplot(combi) +
  geom_histogram(aes(x = combi$contamination), binwidth = 3, fill = "#2565AB", color = "white") +
  ylim(0,450) +
  labs(x = "Checkm Completeness", y = "Count of samples",
       caption = md("Figure 3: histogram of checkm contamination from analysis done by the NCBI at an undisclosed date"),
       title = "Analysis Checkm Completeness")

 +
  geom_text(
    aes(label = completeness),
    position = position_dodge(width = 1),
    vjust = -1)
```

###  {.unnumbered}

The first file, "NCBImetadatacollector.R" creates a directory with all
of the JSON files in it
[here](https://github.com/tobiasnunn/tnunn_research/tree/be8100b850eea50a9e3132806623b4f1d1d08956/02_middle-analysis_outputs/ncbi_stuff/json)
and a file containing the table derived from gtdb-tk,
[here](https://github.com/tobiasnunn/tnunn_research/blob/be8100b850eea50a9e3132806623b4f1d1d08956/02_middle-analysis_outputs/analysis_tables/genera_analysis.tsv).
The second file, "jsondataexplorationprep.R" combines these into
[genera_analysis_combined.R](https://github.com/tobiasnunn/tnunn_research/blob/be8100b850eea50a9e3132806623b4f1d1d08956/02_middle-analysis_outputs/analysis_tables/genera_analysis_combined.tsv).
I used the package "jsonlite" to do this. This does not exist as a code
file, as i did all the coding inside this document. The function
`hoist()` from the package tidyr was also very helpful in drawing out
the lower levels of the JSON files.

I had many attempts at visualising all of the interesting columns but
possibly from messy data, possibly from a lack of understanding the
fields i did not make much progress on that. When trying to display the
information from the "match_status" column i ran into issues relating to
obscure scientific terms. One point of confusion was the reference
genomes, put in the "OK" taxonomy_check_status having a
"below_threshold_mismatch" in the former column. This is the lowest, or
"worst" on the hierarchy listed
[here.](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/policies-annotation/quality/ani/)
This field was especially cryptic, with the hierarchy not working as
this page said it should.

## Conclusion

I do not believe the graphs from this section show anything too
spectacular, however it was good practice at graphing and the main
purpose, being downloading the JSON files so i can make an informed
choice on which accessions will go in my prototype heatmap was a
success. I will be honest, i spent a little too much effort on these
graphs, there were many challanges with doing these things, but i learnt
a lot from that struggle. The next section should however be more
interesting.

::: {style="background-color:yellow;"}
ðŸ“Œ ?: TODO: have a look back at checkm and ANI(if it exists) analysis
for the local samples to add to the combi table in
"jsonataexplorationprep.R" next graphs up are going to be more simple
ones like the one for ANI, and then i also had the idea of doing a sort
of "megagraph" for completeness against contamination, where the colour
is the "refseq category" as for some reason, the accession with the
highest contamination is a reference genome, I can also modify the
column to include "bangor" as the category for ours so I know where they
place.
:::

# Prototype Heatmap Creation Run, end-to-end: step 2 2025-01-26 to 2025-01-2\_

## Introduction

In part 1, i generated the metadata table that will help me choose "high
quality" samples(and maybe go on to make the phylogenetic trees from
gtdb-tk a bit more flashy later). This section will focus on specifying
a set of samples so that each "group" (a subset of samples for each
relevent genus) will total 10 samples, for 50 total (4 samples are
*Sphingomonas,* 2 are *Microbacterium,* there are 1 each for
*Brachybacterium, Brevibacterium* and *Pantoea*). This only totals 9
samples, this is because I am excluding 1Dt100h due to it not being
given a genus by gtdb-tk de_novo_wf. This means i need to select 41 more
samples to download the .fasta file for and copy to hawk.

## Methods

This process will use another command in the NCBI REST API library
called
[-accessions-/download](https://www.ncbi.nlm.nih.gov/datasets/docs/v2/api/rest-api/#get-/genome/accession/-accessions-/download).
This will utilise the combined table made in step 1 to filter to 41
online reference samples of good quality to make the list i will pass to
the API call. My list will not include reference samples because I do
not find that as "organic". On the morning, of the 27th at 8:40 i ran
the NCBI REST API to download the subset of the fastas, using the same
document i started yesterday,
[NCBIfastacollectorprototype.R](https://github.com/tobiasnunn/tnunn_research/blob/c23c334f1fc9acfc10b05f66a0474d6ce3600d3a/00_scripts/NCBIfastacollectorprototype.R).
The code therein place the .fastas in a directory in
02_middle-analysis_outputs/ncbi_stuff. However, this will not show on
github because there are many and fasta files are very large and i have
a limited space allowance in the repo. I then modified this file to
include another for loop to unzip the files and give me the .fastas.

## Results

As mentioned above: the directory containing the .fastas is part of the
.gitignore, so there are no visible results, but trust me, they are
there.

## Conclusion

Non Applicable, this is just a quick intermediary
